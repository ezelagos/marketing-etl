# üöÄ Proyecto de Integraci√≥n de Datos: Marketing ETL Pipeline

## üìã Descripci√≥n del Proyecto
Este proyecto simula un entorno empresarial real implementando un **Pipeline ETL (Extract, Transform, Load)** de extremo a extremo. 

El sistema genera una base de datos ficticia de marketing (simulando campa√±as de ADS), procesa y normaliza los datos, y los carga bajo una estrategia h√≠brida: almacenamiento en **AWS S3** (Data Lake) y persistencia local. Todo el flujo es orquestado autom√°ticamente mediante **Apache Airflow**, garantizando la generaci√≥n diaria de reportes de ventas y m√©tricas clave (KPIs).

### üéØ Objetivos Principales
* **Ingesta de Datos:** Generaci√≥n automatizada de datasets masivos.
* **Transformaci√≥n:** Limpieza y unificaci√≥n de datos con Pandas.
* **Cloud Computing:** Integraci√≥n con AWS S3 para almacenamiento escalable.
* **Orquestaci√≥n:** Automatizaci√≥n de tareas y dependencias con Airflow.
* **An√°lisis:** C√°lculo de m√©tricas de negocio (CPC, CTR, ROI).

## üõ† Tecnolog√≠as Utilizadas

### Infraestructura y Orquestaci√≥n
* ![Ubuntu](https://img.shields.io/badge/Ubuntu_WSL-E95420?style=for-the-badge&logo=ubuntu&logoColor=white) **WSL: Ubuntu 22.04**: Entorno de desarrollo local.
* ![AWS](https://img.shields.io/badge/AWS_S3-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white) **AWS S3**: Almacenamiento en la nube (Data Lake).
* ![Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white) **Apache Airflow**: Orquestaci√≥n de tareas y manejo de dependencias.

### Lenguaje y Librer√≠as (Python)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)

* **Manipulaci√≥n de Datos:** `pandas` üêº (Transformaci√≥n y limpieza).
* **Conexi√≥n Cloud:** `boto3` ‚òÅÔ∏è (SDK de AWS para Python).
* **Generaci√≥n de Datos:** `Faker`, `random`, `uuid` (Simulaci√≥n de datos de marketing realistas).
* **Manejo de Sistema y Tiempo:** `os`, `datetime`.
* **Optimizaci√≥n:** `io.StringIO` (Manejo de buffers en memoria para evitar I/O en disco).

## üèóÔ∏è Arquitectura del Pipeline

El flujo de datos sigue una arquitectura ETL automatizada que conecta un entorno local (WSL) con la nube (AWS).

```mermaid
graph TD;
    A[üêç Generador de Datos] -->|Lotes CSV| B(‚òÅÔ∏è AWS S3: Data Lake);
    B -->|Lectura Raw Data| C[üêº Transformaci√≥n & Unificaci√≥n];
    C -->|Master Dataset| B;
    C -->|Persistencia Local| D[üìÅ Local Storage];
    B -->|Download| E[üìä Motor de An√°lisis];
    E -->|KPIs & M√©tricas| F[üìë Reporte Final];
    
    subgraph Orquestaci√≥n [Apache Airflow DAG]
    A --> C --> E
    end
Flujo de Datos Detallado
Generaci√≥n y Particionamiento:

Se crean datos sint√©ticos de campa√±as de marketing utilizando Faker.

Los datos se generan en particiones/lotes para simular una ingesta masiva y optimizar el uso de memoria RAM (streaming buffers).

Ingesta al Data Lake (S3):

Carga autom√°tica de los archivos crudos (raw data) a un bucket de AWS S3, asegurando la disponibilidad y durabilidad de la informaci√≥n hist√≥rica.

Integraci√≥n (ETL):

El proceso detecta los nuevos archivos en la nube, los descarga y unifica en un Master Dataset.

Se aplica una estrategia de persistencia h√≠brida, guardando el dataset procesado tanto en S3 (para consumo futuro) como en local (para validaci√≥n inmediata).

Automatizaci√≥n con Airflow:

Un DAG (marketing_pipeline_v1) coordina la ejecuci√≥n secuencial de los scripts, manejando dependencias y reintentos ante fallos.

An√°lisis y Reporte:

C√°lculo automatizado de KPIs de negocio: CPC Real, CTR, y tiempo de visualizaci√≥n por audiencia.

Generaci√≥n de un informe final en consola para la toma de decisiones.

üöÄ Instalaci√≥n y Ejecuci√≥n
Sigue estos pasos para desplegar el pipeline en tu entorno local.

Prerrequisitos üìã
Python 3.10 o superior.

Una cuenta de AWS activa con un bucket S3 creado.

Credenciales de AWS (Access Key ID y Secret Access Key) con permisos para S3 (AmazonS3FullAccess o similar).

Paso 1: Clonar el repositorio üì•
Bash

git clone [https://github.com/ezelagos/marketing-etl.git](https://github.com/ezelagos/marketing-etl.git)
cd marketing-etl
Paso 2: Configurar el entorno virtual üêç
Es recomendable usar un entorno virtual para aislar las dependencias.

Bash

# Crear el entorno virtual
python3 -m venv venv

# Activar el entorno
source venv/bin/activate
Paso 3: Instalar dependencias üì¶
Bash

pip install -r requirements.txt
Paso 4: Configurar credenciales de AWS ‚òÅÔ∏è
El proyecto usa boto3 para conectarse a AWS. Configura tus credenciales localmente:

Bash

aws configure
# Ingresa tu AWS Access Key ID, AWS Secret Access Key, y tu regi√≥n por defecto (ej. us-east-1).
Paso 5: Inicializar Airflow üå¨Ô∏è
Configura la base de datos y crea un usuario administrador para Airflow.

Bash

# Inicializar la base de datos
airflow db init

# Crear un usuario admin (cambia los valores seg√∫n prefieras)
airflow users create \
    --username admin \
    --firstname Peter \
    --lastname Parker \
    --role Admin \
    --email spiderman@superhero.org \
    --password admin
Paso 6: Ejecutar el Pipeline ‚ñ∂Ô∏è
Puedes iniciar el servidor web y el scheduler de Airflow en una sola terminal para pruebas locales.

Bash

airflow standalone
Abre tu navegador en http://localhost:8080.

Inicia sesi√≥n con el usuario admin que creaste.

Busca el DAG marketing_pipeline_v1, act√≠valo (ON) y ejec√∫talo (Trigger DAG).



CREADO POR EZEQUIEL LAGOS
