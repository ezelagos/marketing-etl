# üöÄ Proyecto de Integraci√≥n de Datos: Marketing ETL Pipeline

## üìã Descripci√≥n del Proyecto
Este proyecto simula un entorno empresarial real implementando un **Pipeline ETL (Extract, Transform, Load)** de extremo a extremo. 

El sistema genera una base de datos ficticia de marketing (simulando campa√±as de ADS), procesa y normaliza los datos, y los carga bajo una estrategia h√≠brida: almacenamiento en **AWS S3** (Data Lake) y persistencia local. Todo el flujo es orquestado autom√°ticamente mediante **Apache Airflow**, garantizando la generaci√≥n diaria de reportes de ventas y m√©tricas clave (KPIs).

### üéØ Objetivos Principales
* **Ingesta de Datos:** Generaci√≥n automatizada de datasets masivos.
* **Transformaci√≥n:** Limpieza y unificaci√≥n de datos con Pandas.
* **Cloud Computing:** Integraci√≥n con AWS S3 para almacenamiento escalable.
* **Orquestaci√≥n:** Automatizaci√≥n de tareas y dependencias con Airflow.
* **An√°lisis:** C√°lculo de m√©tricas de negocio (CPC, CTR, ROI).

## üõ† Tecnolog√≠as Utilizadas

### Infraestructura y Orquestaci√≥n
* ![Ubuntu](https://img.shields.io/badge/Ubuntu_WSL-E95420?style=for-the-badge&logo=ubuntu&logoColor=white) **WSL: Ubuntu 22.04**: Entorno de desarrollo local.
* ![AWS](https://img.shields.io/badge/AWS_S3-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white) **AWS S3**: Almacenamiento en la nube (Data Lake).
* ![Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white) **Apache Airflow**: Orquestaci√≥n de tareas y manejo de dependencias.

### Lenguaje y Librer√≠as (Python)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)

* **Manipulaci√≥n de Datos:** `pandas` üêº (Transformaci√≥n y limpieza).
* **Conexi√≥n Cloud:** `boto3` ‚òÅÔ∏è (SDK de AWS para Python).
* **Generaci√≥n de Datos:** `Faker`, `random`, `uuid` (Simulaci√≥n de datos de marketing realistas).
* **Manejo de Sistema y Tiempo:** `os`, `datetime`.
* **Optimizaci√≥n:** `io.StringIO` (Manejo de buffers en memoria para evitar I/O en disco).

## üèóÔ∏è Arquitectura del Pipeline

El flujo de datos sigue una arquitectura ETL automatizada que conecta un entorno local (WSL) con la nube (AWS).

```mermaid
graph TD;
    A[üêç Generador de Datos] -->|Lotes CSV| B(‚òÅÔ∏è AWS S3: Data Lake);
    B -->|Lectura Raw Data| C[üêº Transformaci√≥n & Unificaci√≥n];
    C -->|Master Dataset| B;
    C -->|Persistencia Local| D[üìÅ Local Storage];
    B -->|Download| E[üìä Motor de An√°lisis];
    E -->|KPIs & M√©tricas| F[üìë Reporte Final];
    
    subgraph Orquestaci√≥n [Apache Airflow DAG]
    A --> C --> E
    end

¬°Eso est√° genial! Tienes una visi√≥n muy clara del flujo de datos. Has tocado los puntos cr√≠ticos: Particionamiento, Ingesta Cloud, Transformaci√≥n y Entrega de Valor (KPIs).

Para el README.md, vamos a darle un toque visual usando Mermaid (una herramienta que GitHub renderiza autom√°ticamente como un diagrama de flujo) y luego explicaremos tus puntos con un lenguaje t√©cnico pulido.

Aqu√≠ tienes la secci√≥n de Arquitectura lista para copiar. F√≠jate c√≥mo transformamos tus puntos en un flujo profesional:

Markdown

## üèóÔ∏è Arquitectura del Pipeline

El flujo de datos sigue una arquitectura ETL automatizada que conecta un entorno local (WSL) con la nube (AWS).

```mermaid
graph TD;
    A[üêç Generador de Datos] -->|Lotes CSV| B(‚òÅÔ∏è AWS S3: Data Lake);
    B -->|Lectura Raw Data| C[üêº Transformaci√≥n & Unificaci√≥n];
    C -->|Master Dataset| B;
    C -->|Persistencia Local| D[üìÅ Local Storage];
    B -->|Download| E[üìä Motor de An√°lisis];
    E -->|KPIs & M√©tricas| F[üìë Reporte Final];
    
    subgraph Orquestaci√≥n [Apache Airflow DAG]
    A --> C --> E
    end
Flujo de Datos Detallado
Generaci√≥n y Particionamiento:

Se crean datos sint√©ticos de campa√±as de marketing utilizando Faker.

Los datos se generan en particiones/lotes para simular una ingesta masiva y optimizar el uso de memoria RAM (streaming buffers).

Ingesta al Data Lake (S3):

Carga autom√°tica de los archivos crudos (raw data) a un bucket de AWS S3, asegurando la disponibilidad y durabilidad de la informaci√≥n hist√≥rica.

Integraci√≥n (ETL):

El proceso detecta los nuevos archivos en la nube, los descarga y unifica en un Master Dataset.

Se aplica una estrategia de persistencia h√≠brida, guardando el dataset procesado tanto en S3 (para consumo futuro) como en local (para validaci√≥n inmediata).

Automatizaci√≥n con Airflow:

Un DAG (marketing_pipeline_v1) coordina la ejecuci√≥n secuencial de los scripts, manejando dependencias y reintentos ante fallos.

An√°lisis y Reporte:

C√°lculo automatizado de KPIs de negocio: CPC Real, CTR, y tiempo de visualizaci√≥n por audiencia.

Generaci√≥n de un informe final en consola para la toma de decisiones.



## üöÄ Instalaci√≥n y Uso

### Prerrequisitos
* Python 3.10+
* Cuenta de AWS activa (con Access Keys creadas)
* Docker (opcional, si se desea containerizar a futuro)

### 1. Clonar el repositorio
```bash
git clone [https://github.com/TU_USUARIO/nombre-del-repo.git](https://github.com/TU_USUARIO/nombre-del-repo.git)
cd nombre-del-repo


CONFIGURACION ENTORNO VIRTUAL
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

Configurar Credenciales AWS
aws configure
# Ingresa tu Access Key ID, Secret Key y Regi√≥n (ej: us-east-1)

EJECUTAR PIPELINE
iniciar AIRFLOW y EJECUTAR DAG
airflow standalone
# O ejecutar los scripts manualmente para testing:
python3 marketing_data_gen.py
python3 data_merger.py

CREADO POR EZEQUIEL LAGOS